[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables - Assignment 9",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(estimatr)\nlibrary(AER)\n\n#&gt; Loading required package: car\n#&gt; Loading required package: carData\n#&gt; \n#&gt; Attaching package: 'car'\n#&gt; \n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     recode\n#&gt; \n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     some\n#&gt; \n#&gt; Loading required package: lmtest\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; \n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n#&gt; \n#&gt; Loading required package: sandwich\n#&gt; Loading required package: survival\n\n\n\n1 Part1\n\nDefine the DAG \n\ndag &lt;- dagify(\n  appTime ~ newFeatureUsed,\n  appTime ~ Unobserved,\n  newFeatureUsed ~ Unobserved,\n  newFeatureUsed ~ userEncouragement,\n  exposure = \"newFeatureUsed\",\n  latent = \"Unobserved\",\n  outcome = \"appTime\",\n  coords = list(x = c(Unobserved = 1, newFeatureUsed = 0, appTime = 2, userEncouragement = -1),\n                y = c(Unobserved = 1, newFeatureUsed = 0, appTime = 0, userEncouragement = 0)),\n  labels = c(\n    \"appTime\" = \"Time Spent on the App\",\n    \"newFeatureUsed\" = \"The new feature is used\",\n    \"userEncouragement\" = \"User encouragement to use feature\",\n    \"Unobserved\" = \"Unobserved variables\"\n  )\n)\n\n\n\nPlot DAG \n\nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n\n\n\n\n2 Part2\n\nLoad the Data \n\nappData &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/rand_enc.rds\")\n\n\n\nBe familiar with the data \n\nhead(appData)\n\n\n\n  \n\n\n\n\n\nCompute Naive Estimate \n\nnaiveEstimate &lt;- lm(time_spent ~ used_ftr , data = appData)\nsummary(naiveEstimate)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = appData)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n3 Part3\n\nCheck the correlation matrix\n\ncorrelationMatrix &lt;- cor(appData) %&gt;% round(2)\n\n\n\n\n\n\n\nThere is an upward bias in the naive estimate (10.82269) as it surpasses the IV robust estimate (9.738175) when utilizing rand_enc as an instrument. This implies that the effect of used_ftr on time_spent has been overestimated. It is clear that used_ftr and time_spent have a strong correlation. Given that rand_enc shows a weak correlation with time_spent and a higher correlation with treatment (used_ftr), it makes sense to regard it as an instrumental variable. The instrumental variable and outcome have a reasonably low correlation, even though it is not 0 (maybe because of noise).\n\n\n\n\n\n\n4 Part4\n\nInstrumental Variable Estimation using 2SLS with rand_enc and robust standard errors\n\nivModelRobust &lt;- iv_robust(time_spent ~ used_ftr  | rand_enc, data = appData)\nsummary(ivModelRobust)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = appData)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nHansen J test\n\n\nExtract residuals and fitted values from the model\n\nresidualsIV &lt;- residuals(ivModelRobust)\nfittedValuesIV &lt;- fitted(ivModelRobust)\n\n\n\nPerform Hansen J test\n\nhansenTestStat &lt;- sum(residualsIV * fittedValuesIV)\npValueHansen &lt;- 1 - pchisq(hansenTestStat, df = 1)\n\n\n\nDisplay results\n\n\nCode\ncat(\"Hansen J Test Statistic:\", hansenTestStat, \"\\n\")\n\n\n#&gt; Hansen J Test Statistic: 0\n\n\nCode\ncat(\"P-value:\", pValueHansen, \"\\n\")\n\n\n#&gt; P-value: 1\n\n\n\n\n\n\n\n\nA Hansen J test with a test statistic near 0 and a p-value near 1 suggests that the model’s instrument is not violating the over-identifying constraints. In other words, there is no evidence to suggest that the instrument is endogenous or linked with the error term, and the instrument is valid for the model.\n\n\n\n\n\nCode\ncat(\"Naive Estimate:\", coef(naiveEstimate)['used_ftr'], \"\\n\")\n\n\n#&gt; Naive Estimate: 10.82269\n\n\nCode\ncat(\"IV Robust Estimate (rand_enc):\", ivModelRobust$coefficients['used_ftr'], \"\\n\")\n\n\n#&gt; IV Robust Estimate (rand_enc): 9.738175\n\n\n\n\n\n\n\n\nWe would consider the naive estimate to have an upward bias because it is larger than the IV robust estimate using rand_enc as an instrument (9.738175). This suggests that the effect of usedFeature on appTime is overestimated by the naive estimate."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification - Assignment 7",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(ggplot2)  # Make sure to load ggplot2\nlibrary(MatchIt)\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n\n\n# Load data\ncustomer_data &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/membership.rds\")\n\n\n1 Part1\n\n# Explore the data and check relationships between variables\nsummary(customer_data)\n\n#&gt;       age             sex         pre_avg_purch         card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :-14.23   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.82   1st Qu.:0.0000  \n#&gt;  Median :38.80   Median :1.0000   Median : 70.15   Median :0.0000  \n#&gt;  Mean   :40.37   Mean   :0.5038   Mean   : 70.42   Mean   :0.4232  \n#&gt;  3rd Qu.:49.20   3rd Qu.:1.0000   3rd Qu.: 88.79   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.42   Max.   :1.0000  \n#&gt;    avg_purch     \n#&gt;  Min.   :-28.61  \n#&gt;  1st Qu.: 54.02  \n#&gt;  Median : 76.24  \n#&gt;  Mean   : 76.61  \n#&gt;  3rd Qu.: 98.54  \n#&gt;  Max.   :192.91\n\ncor(customer_data)\n\n#&gt;                      age          sex pre_avg_purch        card   avg_purch\n#&gt; age           1.00000000  0.012532675   0.517506430 0.105533628 0.448632638\n#&gt; sex           0.01253267  1.000000000  -0.001221386 0.008468092 0.002181853\n#&gt; pre_avg_purch 0.51750643 -0.001221386   1.000000000 0.192333327 0.855828507\n#&gt; card          0.10553363  0.008468092   0.192333327 1.000000000 0.382352233\n#&gt; avg_purch     0.44863264  0.002181853   0.855828507 0.382352233 1.000000000\n\n# Create DAG using dagitty\ncollider &lt;- dagitty('dag {\n  avg_purch &lt;- plus_membership\n  avg_purch &lt;- age\n  avg_purch &lt;- sex\n  avg_purch &lt;- pre_avg_purch\n}')\n\n# Plot DAG using ggdag\nggdag(collider) +\n  geom_dag_point() +\n  geom_dag_text(color = \"blue\") +\n  geom_dag_edges(edge_color = \"red\")\n\n\n\n\n\n\n\n\n\n\n2 Part2\n\nNaive estimation \n\n# Naive estimation \nmodel_naive &lt;- lm(avg_purch ~ card   , data = customer_data)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n3 Part3.1\n\n(Coarsened) Exact Matching \n\ncem &lt;- matchit(card ~ age + pre_avg_purch+sex,\n               data = customer_data, \n               method = 'cem', \n               estimand = 'ATE')\n# Covariate balance\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = customer_data, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt; sex                  0.5040        0.5040          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0064          0.1222\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\n# Use matched data\ndf_cem &lt;- match.data(cem)\n\n# (2) Estimation\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n4 Part3.2\n\nNearest-Neighbor Matching \n\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn &lt;- matchit(card ~ age + pre_avg_purch+sex,\n              data = customer_data,\n              method = \"nearest\", # changed\n              distance = \"mahalanobis\", # changed\n              replace = T)\n\n# Covariate Balance\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = customer_data, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n5 Part3.3\n\nInverse Probability Weighting \n\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch+sex,\n                  data = customer_data,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch + sex, family = binomial(link = \"logit\"), \n#&gt;     data = customer_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Add propensities to table\ndf_aug &lt;- customer_data %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\ndf_aug\n\n\n\n  \n\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\ndf_ipw\n\n\n\n  \n\n\n# Look at data with IPW scores\ndf_ipw %&gt;% \n  select(card, age, pre_avg_purch,sex, propensity, ipw)\n\n\n\n  \n\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch  ~ card ,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs - Assignment 5",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)    # For data manipulation and visualization\nlibrary(dagitty)      # For working with Directed Acyclic Graphs (DAGs)\nlibrary(ggdag)        # For DAG visualization with ggplot2\n\n\n1 Part1\n\nDraw DAG\n\n# Construct DAG for Sales Example\nsales_graph &lt;- dagify(\n  product_sales ~ parking_capacity,\n  product_sales ~ store_location,\n  parking_capacity ~ store_location, # store_location acts as the confounding variable\n  labels = c(\n    \"product_sales\" = \"Product Sales\",\n    \"parking_capacity\" = \"Parking \\n Capacity\",\n    \"store_location\" = \"Store Location\"\n  )\n)\nggdag(sales_graph, use_labels = \"label\", text = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n2 Part2\n\nRead and Explore Data\n\n# Load the customer satisfaction data\ncustomer_data &lt;- readRDS('C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/customer_sat.rds')\n# Explore the data\nhead(customer_data)\n\n\n\n  \n\n\n\n\n\n\n3 Part2.1\n\nFit a regression model: satisfaction ~ follow-ups\n\n## Simple linear Regression ##\nmodel_unadjusted &lt;- lm(satisfaction ~ follow_ups, data = customer_data)\n\nsummary(model_unadjusted)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n\n\n\n4 Part2.2\n\nFit a regression model: satisfaction ~ follow-ups + subscription\n\n## Multiple Linear Regression\nmodel_adjusted &lt;- lm(satisfaction ~ follow_ups + subscription, data = customer_data)\n\nsummary(model_adjusted)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n\n5 Part3\n\nCompare coefficients from the two models\n\ncoef_comparison &lt;- data.frame(\n  Model = c(\"Model 1\", \"Model 2\"),\n  Intercept = c(coef(model_unadjusted)[1], coef(model_adjusted)[1]),\n  FollowUps = c(coef(model_unadjusted)[2], coef(model_adjusted)[2]),\n  PremiumPlus = c(0, coef(model_adjusted)[grep(\"subscription_typePremium\\\\+\", names(coef(model_adjusted)))]),\n  Elite = c(0, coef(model_adjusted)[grep(\"subscription_typeElite\", names(coef(model_adjusted)))])\n)\n\nprint(coef_comparison)\n\n#&gt;     Model Intercept FollowUps PremiumPlus Elite\n#&gt; 1 Model 1  78.88605 -3.309302           0     0\n#&gt; 2 Model 2  26.76667  2.194444           0     0\n\n\n\n\n\n\n\n\nModel 2 exhibits a lower baseline satisfaction compared to Model 1, attributed to its reduced intercept. Incorporating subscription information altered the direction of the association between Follow-ups and satisfaction, shifting it from negative to positive. PremiumPlus subscription has a positive influence on satisfaction, while there is no supplementary effect observed for the Elite subscription level.\n\n\n\n\n\n\n6 Part4\n\nVisualise Data\n\n### Simpson's Paradox: Subscription as the Confounding Variable ###\n\n\n## Without conditioning on subscription\n\nsimps_not_cond &lt;- ggplot(customer_data, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F) +\n  labs(title = \"Relationship between Follow-ups and Satisfaction\",\n       x = \"Follow-up Calls\",\n       y = \"Customer Satisfaction\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n## Conditioning on subscription\n\nsimps_cond &lt;- ggplot(customer_data, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F, size = 1) +\n  labs(title = \"Relationship between Follow-ups and Satisfaction by Subscription Type\",\n       x = \"Follow-up Calls\",\n       y = \"Customer Satisfaction\",\n       color = \"Subscription Type\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\nPart 4 Results\n\n\nCode\nsimps_not_cond\n\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nsimps_cond\n\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference - Assignment 3",
    "section": "",
    "text": "# Load Libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(modelr)\n\n\n# Load data\n# Read data set. Check what folder/path your data is in. \ncar_data &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/car_prices.rds\")\n\n\n1 Part1\n\nNumber of rows and columns\n\nrows_columns &lt;- dim(car_data)\nrows &lt;- rows_columns[1]\ncolumns &lt;- rows_columns[2]\n\n\n\nNumber of rows and columns\n\n\nCode\ncat(\"Number of rows:\", rows, \"\\n\")\n\n\n#&gt; Number of rows: 181\n\n\nCode\ncat(\"Number of columns:\", columns, \"\\n\")\n\n\n#&gt; Number of columns: 22\n\n\n\n\n\n2 Part2\n\n# Extract column names for each data frame in the list\ncolumn_names_list &lt;- lapply(car_data, colnames)\nfirst_element &lt;- head(car_data$carwidth, 1)\ncat(\"the type of carwidth is\", typeof(first_element), \"\\n\")\n\n#&gt; the type of carwidth is double\n\nfirst_element &lt;- head(car_data$carbody, 1)\ncat(\"the type of carbody is\", typeof(first_element), \"\\n\")\n\n#&gt; the type of carbody is character\n\nprint(\"There are two types: char and double\")\n\n#&gt; [1] \"There are two types: char and double\"\n\n\n\n\n\n\n\n\nNumeric types like double are ideal for quantitative information, while strings and char are useful for textual and categorical information. The key is to accurately represent the real-world attributes of the cars and associated market data in your program.\n\n\n\n\n\n3 Part3\n\nLinear Regression\n\nglimpse(car_data)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\ncar_data[] &lt;- lapply(car_data, as.numeric)\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_data, as.numeric): NAs introduced by coercion\n\ncar_data %&gt;%\n  # Calculate the correlation matrix\n  cor() %&gt;% \n  # Round the correlation values to 2 decimal places\n  round(2) %&gt;% \n  # Extract the lower triangular part of the matrix\n  Matrix::tril()\n\n#&gt; 22 x 22 Matrix of class \"dtrMatrix\"\n#&gt;                  aspiration doornumber carbody drivewheel enginelocation\n#&gt; aspiration             1.00          .       .          .              .\n#&gt; doornumber               NA       1.00       .          .              .\n#&gt; carbody                  NA         NA    1.00          .              .\n#&gt; drivewheel               NA         NA      NA       1.00              .\n#&gt; enginelocation           NA         NA      NA         NA           1.00\n#&gt; wheelbase                NA         NA      NA         NA             NA\n#&gt; carlength                NA         NA      NA         NA             NA\n#&gt; carwidth                 NA         NA      NA         NA             NA\n#&gt; carheight                NA         NA      NA         NA             NA\n#&gt; curbweight               NA         NA      NA         NA             NA\n#&gt; enginetype               NA         NA      NA         NA             NA\n#&gt; cylindernumber           NA         NA      NA         NA             NA\n#&gt; enginesize               NA         NA      NA         NA             NA\n#&gt; fuelsystem               NA         NA      NA         NA             NA\n#&gt; boreratio                NA         NA      NA         NA             NA\n#&gt; stroke                   NA         NA      NA         NA             NA\n#&gt; compressionratio         NA         NA      NA         NA             NA\n#&gt; horsepower               NA         NA      NA         NA             NA\n#&gt; peakrpm                  NA         NA      NA         NA             NA\n#&gt; citympg                  NA         NA      NA         NA             NA\n#&gt; highwaympg               NA         NA      NA         NA             NA\n#&gt; price                    NA         NA      NA         NA             NA\n#&gt;                  wheelbase carlength carwidth carheight curbweight enginetype\n#&gt; aspiration               .         .        .         .          .          .\n#&gt; doornumber               .         .        .         .          .          .\n#&gt; carbody                  .         .        .         .          .          .\n#&gt; drivewheel               .         .        .         .          .          .\n#&gt; enginelocation           .         .        .         .          .          .\n#&gt; wheelbase             1.00         .        .         .          .          .\n#&gt; carlength             0.86      1.00        .         .          .          .\n#&gt; carwidth              0.77      0.83     1.00         .          .          .\n#&gt; carheight             0.54      0.44     0.20      1.00          .          .\n#&gt; curbweight            0.74      0.87     0.85      0.21       1.00          .\n#&gt; enginetype              NA        NA       NA        NA         NA       1.00\n#&gt; cylindernumber          NA        NA       NA        NA         NA         NA\n#&gt; enginesize            0.55      0.68     0.74     -0.02       0.87         NA\n#&gt; fuelsystem              NA        NA       NA        NA         NA         NA\n#&gt; boreratio             0.46      0.60     0.55      0.14       0.64         NA\n#&gt; stroke                0.07      0.07     0.11     -0.15       0.10         NA\n#&gt; compressionratio     -0.26     -0.25    -0.25     -0.05      -0.31         NA\n#&gt; horsepower            0.40      0.60     0.70     -0.09       0.82         NA\n#&gt; peakrpm              -0.22     -0.19    -0.11     -0.15      -0.16         NA\n#&gt; citympg              -0.58     -0.78    -0.74     -0.15      -0.87         NA\n#&gt; highwaympg           -0.63     -0.79    -0.75     -0.18      -0.89         NA\n#&gt; price                 0.56      0.67     0.74      0.07       0.83         NA\n#&gt;                  cylindernumber enginesize fuelsystem boreratio stroke\n#&gt; aspiration                    .          .          .         .      .\n#&gt; doornumber                    .          .          .         .      .\n#&gt; carbody                       .          .          .         .      .\n#&gt; drivewheel                    .          .          .         .      .\n#&gt; enginelocation                .          .          .         .      .\n#&gt; wheelbase                     .          .          .         .      .\n#&gt; carlength                     .          .          .         .      .\n#&gt; carwidth                      .          .          .         .      .\n#&gt; carheight                     .          .          .         .      .\n#&gt; curbweight                    .          .          .         .      .\n#&gt; enginetype                    .          .          .         .      .\n#&gt; cylindernumber             1.00          .          .         .      .\n#&gt; enginesize                   NA       1.00          .         .      .\n#&gt; fuelsystem                   NA         NA       1.00         .      .\n#&gt; boreratio                    NA       0.58         NA      1.00      .\n#&gt; stroke                       NA       0.18         NA     -0.10   1.00\n#&gt; compressionratio             NA      -0.16         NA     -0.20  -0.30\n#&gt; horsepower                   NA       0.85         NA      0.59   0.11\n#&gt; peakrpm                      NA      -0.18         NA     -0.24   0.07\n#&gt; citympg                      NA      -0.74         NA     -0.62  -0.09\n#&gt; highwaympg                   NA      -0.76         NA     -0.61  -0.07\n#&gt; price                        NA       0.89         NA      0.55   0.03\n#&gt;                  compressionratio horsepower peakrpm citympg highwaympg price\n#&gt; aspiration                      .          .       .       .          .     .\n#&gt; doornumber                      .          .       .       .          .     .\n#&gt; carbody                         .          .       .       .          .     .\n#&gt; drivewheel                      .          .       .       .          .     .\n#&gt; enginelocation                  .          .       .       .          .     .\n#&gt; wheelbase                       .          .       .       .          .     .\n#&gt; carlength                       .          .       .       .          .     .\n#&gt; carwidth                        .          .       .       .          .     .\n#&gt; carheight                       .          .       .       .          .     .\n#&gt; curbweight                      .          .       .       .          .     .\n#&gt; enginetype                      .          .       .       .          .     .\n#&gt; cylindernumber                  .          .       .       .          .     .\n#&gt; enginesize                      .          .       .       .          .     .\n#&gt; fuelsystem                      .          .       .       .          .     .\n#&gt; boreratio                       .          .       .       .          .     .\n#&gt; stroke                          .          .       .       .          .     .\n#&gt; compressionratio             1.00          .       .       .          .     .\n#&gt; horsepower                  -0.22       1.00       .       .          .     .\n#&gt; peakrpm                      0.16       0.08    1.00       .          .     .\n#&gt; citympg                      0.44      -0.81    0.03    1.00          .     .\n#&gt; highwaympg                   0.45      -0.78    0.05    0.98       1.00     .\n#&gt; price                       -0.18       0.84   -0.02   -0.74      -0.74  1.00\n\n\n\n\n\n4 Part4.1\n\nexplain what data type it is and what values it can take on\n\nfirst_element &lt;- head(car_data$enginesize, 1)\nmin_value &lt;- min(car_data$enginesize)\nmax_value &lt;- max(car_data$enginesize)\ncat(\"The regressor chosen is the enginesize and it has a type of \", typeof(first_element),\" and it ranges between\" ,min_value,\" and \", max_value ,\"\\n\")\n\n#&gt; The regressor chosen is the enginesize and it has a type of  double  and it ranges between 61  and  326\n\n\n\n\n\n5 Part4.2\n\n\n\n\n\n\nIt affects the price with a percentage of 89% so it has a big effect, and it effects it with a positive correlation so when the engine size increases the price increases (in a proportional manner).\n\n\n\n\n\n6 Part4.3\n\n\n\n\n\n\nAs mentioned it affects the price with a 89% , so it does affect it significantly\n\n\n\n\n\n7 Part5\n\nSeat Heating\n\ncardata_with_seat_heating &lt;- car_data %&gt;%\n  mutate(seat_heatingTRUE = sample(c(TRUE, FALSE), size = nrow(car_data), replace = TRUE))\n\n# Check the distribution of seat_heatingTRUE\ntable(cardata_with_seat_heating$seat_heatingTRUE)\n\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;    78   103\n\n# Run the regression again\nlinear_model &lt;- lm(price ~ seat_heatingTRUE, data = cardata_with_seat_heating)\n\n# Print the summary of the linear model\nsummary(linear_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ seat_heatingTRUE, data = cardata_with_seat_heating)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -9485  -5068  -2812   3644  33640 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           14635.8      901.6  16.233   &lt;2e-16 ***\n#&gt; seat_heatingTRUETRUE  -2875.7     1195.2  -2.406   0.0171 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7963 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.03133,    Adjusted R-squared:  0.02592 \n#&gt; F-statistic: 5.789 on 1 and 179 DF,  p-value: 0.01715\n\n\n\n\n\n\n\n\nIf all the values are assigned as true there will be no variability, and which can lead to issues in regression analysis, including collinearity problems and difficulties in estimating coefficients."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability- Assignment1",
    "section": "",
    "text": "# Load Libraries\n\nlibrary(tidyverse)\n\n\n1 Assignment1\n\n# Define probabilities\nP_S &lt;- 0.3\nP_T_S &lt;- 0.2\nP_T_S_Bar &lt;- 0.6\n\n# Calculate complementary probabilities\nP_S_Bar &lt;- 1 - P_S\nP_T_Bar_S &lt;- 1 - P_T_S\nP_T_Bar_S_Bar &lt;- 1 - P_T_S_Bar\n\n# Calculate conditional probabilities\nP_T_inter_S &lt;- P_S * P_T_S\nP_T_inter_S_Bar &lt;- P_S_Bar * P_T_S_Bar\nP_T_Bar_inter_S &lt;- P_S * P_T_Bar_S\nP_T_Bar_inter_S_Bar &lt;- P_S_Bar * P_T_Bar_S_Bar\n\n\nAssignment1 Results\n\n\nCode\ncat(\"P_T_inter_S:\",P_T_inter_S)\n\n\n#&gt; P_T_inter_S: 0.06\n\n\nCode\ncat(\"P_T_inter_S_Bar\",P_T_inter_S_Bar)\n\n\n#&gt; P_T_inter_S_Bar 0.42\n\n\nCode\ncat(\"P_T_Bar_inter_S\",P_T_Bar_inter_S)\n\n\n#&gt; P_T_Bar_inter_S 0.24\n\n\nCode\ncat(\"P_T_Bar_inter_S_Bar\",P_T_Bar_inter_S_Bar)\n\n\n#&gt; P_T_Bar_inter_S_Bar 0.28\n\n\n\n\n\n2 Assignment2\n\nFill in the Dataset\n\n# Number of obervations\nn &lt;- 1000\n\n# Create tibble\napp_usage &lt;- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n# If no device has value of 1, we set smartphone to 1\napp_usage &lt;- app_usage %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n\n\nPreview of the Dataset\n\n\nCode\n# Show first ten lines\nhead(app_usage, 10)\n\n\n\n\n  \n\n\n\n\n\nShow column sums\n\n\nCode\ncolSums(app_usage)\n\n\n#&gt;    user_id smartphone     tablet   computer \n#&gt;     500500        598        377        235\n\n\n\n\nCreate the sets needed\n\n# Set of phone, tablet and computer users\nset_phon &lt;- which(app_usage$smartphone == 1)\nset_tabl &lt;- which(app_usage$tablet == 1)\nset_comp &lt;- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all &lt;- list(set_phon, set_tabl, set_comp)\n\n\n\nLoad additional package for plotting Venn diagrams\n\nlibrary(ggVennDiagram)\n\n\n\nPlot Venn Diagram\n\n\nCode\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\n\n\n\n\nAssignment 2 Results\n\n# Calculate the intersection of sets (users using all three devices)\nset_all_three &lt;- Reduce(intersect, sets_all)\npercentage_all_three &lt;- length(set_all_three) / n * 100\ncat(\"Percentage of customers using all three devices:\", round(percentage_all_three, 2), \"%\\n\")\n\n#&gt; Percentage of customers using all three devices: 0.4 %\n\n\n\n# Calculate the union of sets (users using at least two devices)\nset_at_least_two &lt;- union(intersect(set_phon, set_tabl), union(intersect(set_phon, set_comp), intersect(set_comp, set_tabl)))\npercentage_at_least_two &lt;- length(set_at_least_two) / n * 100\ncat(\"Percentage of customers using at least two devices:\", round(percentage_at_least_two, 2), \"%\\n\")\n\n#&gt; Percentage of customers using at least two devices: 20.6 %\n\n\n\n# Calculate the sets of users using each device exclusively\nset_only_phon &lt;- set_phon[!(set_phon %in% set_tabl) & !(set_phon %in% set_comp)]\nset_only_tabl &lt;- set_tabl[!(set_tabl %in% set_phon) & !(set_tabl %in% set_comp)]\nset_only_comp &lt;- set_comp[!(set_comp %in% set_phon) & !(set_comp %in% set_tabl)]\npercentage_only_one &lt;- (length(set_only_phon) + length(set_only_tabl) + length(set_only_comp)) / n * 100\ncat(\"Percentage of customers using only one device:\", round(percentage_only_one, 2), \"%\\n\")\n\n#&gt; Percentage of customers using only one device: 79.4 %\n\n\n\n\n\n3 Assignment2\n\nInitialize data\n\nP_A &lt;- 0.04\nP_B_A &lt;- 0.97\nP_B_A_Bar &lt;- 0.99\n\n\n\nCalculation\n\nP_A_Bar &lt;- 1- P_A\nP_B_Bar_A &lt;- 1 - P_B_A\nP_B_Bar_A_Bar &lt;- 1 - P_B_A_Bar\nP_B &lt;- (P_B_A*P_A)+(P_B_A_Bar*P_A_Bar)\nP_A_Bar_knowing_B &lt;- (P_B_A_Bar * P_A_Bar)/P_B\nP_A_knowing_B &lt;- (P_B_A * P_A)/P_B\n\n\n\nAssignment3 Results\n\nprint(P_A_Bar_knowing_B)\n\n#&gt; [1] 0.9607764\n\nprint(P_A_knowing_B)\n\n#&gt; [1] 0.03922362"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts- Assignment2",
    "section": "",
    "text": "# Load data\nrandom_vars &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/random_vars.rds\")\n\n\n1 Part1\n\nSet age and icome sets\n\nage_var &lt;- random_vars$age\nincome_var &lt;- random_vars$income\n\n\n\n\n2 Part1.1\n\n# 1.Expected Value\nexpected_value_age &lt;- sum(age_var) / length(age_var)\nexpected_value_income &lt;- sum(income_var) / length(income_var)\n\n\nPart1.1 Results\n\n\nCode\ncat(\"Expected age Value:\", expected_value_age, \"\\n\")\n\n\n#&gt; Expected age Value: 33.471\n\n\nCode\ncat(\"Expected income Value:\", expected_value_income, \"\\n\")\n\n\n#&gt; Expected income Value: 3510.731\n\n\n\n\n\n3 Part1.2\n\n#2.Variance\nmean_value_age &lt;- mean(age_var)\nmean_value_income &lt;- mean(income_var)\nvariance_value_age &lt;- sum((age_var - mean_value_age)^2) / (length(age_var) - 1)\nvariance_value_income&lt;- sum((income_var - mean_value_income)^2) / (length(income_var) - 1)\n\n\nPart1.2 Results\n\n\nCode\ncat(\"Variance age Value:\", variance_value_age, \"\\n\")\n\n\n#&gt; Variance age Value: 340.6078\n\n\nCode\ncat(\"Variance income Value:\", variance_value_income, \"\\n\")\n\n\n#&gt; Variance income Value: 8625646\n\n\n\n\n\n4 Part1.3\n\n#3.Standard Deviation\nstandard_deviation_value_age &lt;- sqrt(variance_value_age)\nstandard_deviation_value_income &lt;- sqrt(variance_value_income)\n\n\nPart1.3 Results\n\n\nCode\ncat(\"Standard Deviation age Value:\", standard_deviation_value_age, \"\\n\")\n\n\n#&gt; Standard Deviation age Value: 18.45556\n\n\nCode\ncat(\"Standard Deviation income Value:\", standard_deviation_value_income, \"\\n\")\n\n\n#&gt; Standard Deviation income Value: 2936.945\n\n\n\n\n\n5 Part2\n\nComparing Standard Deviation\n\n\n\n\n\n\nComparing standard deviations directly is most meaningful when the variables are measured in the same units and have similar scales. In the case of age and income, these two variables have different units and different scales. Therefore, comparing their standard deviations directly may not provide meaningful insights.\n\n\n\n\n\n\n6 Part3.1\n\nCovariance\n\ncovariance_value &lt;- sum((age_var - mean_value_age) * (income_var - mean_value_income)) / length(age_var)\n\n\n\nPart3.1 Results\n\n\nCode\ncat(\"Covariance Value:\", covariance_value, \"\\n\")\n\n\n#&gt; Covariance Value: 29670.45\n\n\n\n\n\n7 Part3.2\n\nCorrelation\n\ncorrelation_value &lt;- covariance_value / (standard_deviation_value_age*standard_deviation_value_income)\n\n\n\nPart3.1 Results\n\n\nCode\ncat(\"Correlation Value:\", correlation_value, \"\\n\")\n\n\n#&gt; Correlation Value: 0.5473952\n\n\n\n\n\n8 Part4\n\n\n\n\n\n\nThe correlation coefficient’s standardized scale makes it more intuitive for comparison. The fact that it ranges from -1 to 1 allows for a clear understanding of the strength and direction of the relationship.\n\n\n\n\n\n9 Part5.1\n\nconditional expected value, E[income|age&lt;=18]\n\nsubset_data1 &lt;- subset(random_vars, age &lt;= 18)\nconditional_expected_value1 &lt;- mean(subset_data1$income)\n\n\n\nPart5.1 Results\n\n\nCode\ncat(\"Conditional Expected Value of Income for age &lt;= 18:\", conditional_expected_value1, \"\\n\")\n\n\n#&gt; Conditional Expected Value of Income for age &lt;= 18: 389.6074\n\n\n\n\n\n10 Part5.2\n\nconditional expected value, E[income|age&lt;=[18,65)]\n\nsubset_data2 &lt;- subset(random_vars, age &gt;= 18 & age &lt; 65)\nconditional_expected_value2 &lt;- mean(subset_data2$income)\n\n\n\nPart5.2 Results\n\n\nCode\ncat(\"Conditional Expected Value of Income for age in [18, 65):\", conditional_expected_value2, \"\\n\")\n\n\n#&gt; Conditional Expected Value of Income for age in [18, 65): 4685.734\n\n\n\n\n\n11 Part5.2\n\nconditional expected value, E[income|age&gt;=65]\n\nsubset_data3 &lt;- subset(random_vars, age &gt;= 65)\nconditional_expected_value3 &lt;- mean(subset_data3$income)\n\n\n\nCode\ncat(\"Conditional Expected Value of Income for age &gt;= 65:\", conditional_expected_value3, \"\\n\")\n\n\n#&gt; Conditional Expected Value of Income for age &gt;= 65: 1777.237"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality - Assignment 4",
    "section": "",
    "text": "the correlation between the consumption of chocolate and the number of Nobel laureates in a country. While these two variables might seem unrelated, there could be a spurious correlation due to a common third variable, such as the country’s level of\n\n\n\n\n# Generate a sample dataset (replace this with your actual dataset)\nset.seed(789)\ncountries &lt;- c(\"USA\", \"Sweden\", \"Switzerland\", \"Belgium\", \"Norway\")\nchocolate_consumption &lt;- rnorm(length(countries), mean = 5, sd = 2)\nnobel_laureates &lt;- rpois(length(countries), lambda = 5)\neducation_years &lt;- rnorm(length(countries), mean = 12, sd = 2)\n\n# Create a dataframe\ndata &lt;- data.frame(Country = countries, ChocolateConsumption = chocolate_consumption, NobelLaureates = nobel_laureates, EducationYears = education_years)\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Plotting\nggplot(data, aes(x = ChocolateConsumption, y = NobelLaureates, color = EducationYears)) +\n  geom_point() +\n  labs(title = \"Spurious Correlation: Chocolate Consumption vs Nobel Laureates\",\n       x = \"Chocolate Consumption\",\n       y = \"Nobel Laureates\",\n       color = \"Education Years\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#In this example, the scatter plot is colored by the average years of education in each country. The idea is to highlight #that the correlation between chocolate consumption and Nobel laureates may be influenced by the level of education in a country."
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials - Assignment 6",
    "section": "",
    "text": "1 Part1\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n\ncustomer_data &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/abtest_online.rds\")\ncustomer_data\n\n\n\n  \n\n\n\n\ncompare_purchase_amount &lt;- \n  ggplot(customer_data, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"purchase_amount\", title = \"Difference in purchase amount\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\ncompare_previous_visit &lt;- \n  ggplot(customer_data, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"previous_visit\", title = \"Difference in previous visit\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\ncompare_mobile_device &lt;- \n  ggplot(customer_data, \n         aes(x = chatbot, \n             y = mobile_device, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"mobile_device\", title = \"Difference in mobile device\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\ncompare_purchase &lt;- \n  ggplot(customer_data, \n         aes(x = chatbot,\n             y = purchase, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"purchase\", title = \"Difference in purchase\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\n\n# Plot \ncompare_purchase_amount\n\n\n\n\n\n\n\ncompare_previous_visit\n\n\n\n\n\n\n\ncompare_mobile_device\n\n\n\n\n\n\n\ncompare_purchase\n\n\n\n\n\n\n\n\n\n\n2 Part2\n\nlm1 &lt;- lm(purchase ~ chatbot, data = customer_data)\nsummary(lm1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.4960 -0.3249 -0.2679  0.5040  0.7321 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.49597    0.02122  23.376  &lt; 2e-16 ***\n#&gt; chatbotTRUE -0.22811    0.02989  -7.633 5.36e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4725 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.05516,    Adjusted R-squared:  0.05421 \n#&gt; F-statistic: 58.26 on 1 and 998 DF,  p-value: 5.36e-14\n\nlm2 &lt;- lm(purchase_amount ~ chatbot, data = customer_data)\nsummary(lm2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\n\n\n3 Part3\n\nlm_mod1 &lt;- lm(purchase ~ chatbot * mobile_device, data = customer_data)\nsummary(lm_mod1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot * mobile_device, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.5000 -0.3284 -0.2635  0.5000  0.7484 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    0.50000    0.02572  19.438  &lt; 2e-16 ***\n#&gt; chatbotTRUE                   -0.22464    0.03619  -6.207 7.93e-10 ***\n#&gt; mobile_deviceTRUE             -0.01266    0.04558  -0.278    0.781    \n#&gt; chatbotTRUE:mobile_deviceTRUE -0.01113    0.06428  -0.173    0.863    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4729 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.05549,    Adjusted R-squared:  0.05264 \n#&gt; F-statistic:  19.5 on 3 and 996 DF,  p-value: 2.714e-12\n\nlm_mod2 &lt;- lm(purchase_amount ~ chatbot * mobile_device, data = customer_data)\nsummary(lm_mod2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbotTRUE                    -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_deviceTRUE              -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbotTRUE:mobile_deviceTRUE  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08\n\n\n\n\n4 Part4\n\n# Logistic regression model\nlogistic_model &lt;- glm(purchase ~ chatbot, family = binomial(link = 'logit'), data = customer_data)\n\n# Display summary of the logistic regression model\nsummary(logistic_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = customer_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbotTRUE -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\nThe chatbotTRUE coefficient is of particular interest. Since it has a negative estimate, it suggests that, compared to when the chatbot is not present, the presence of the chatbot is associated with a decrease in the log-odds of making a purchase. The p-value being less than 0.001 indicates that this effect is statistically significant. In simpler terms, the model suggests that customers are less likely to make a purchase when the chatbot is present, and this effect is unlikely to be due to random chance. Lastly, the null and residual deviances and the AIC (Akaike Information Criterion) are model fit statistics. Lower AIC values generally indicate better-fitting models. In this case, the residual deviance is 1273.3 on 998 degrees of freedom, and the AIC is 1277.3"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences - Assignment 8",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(lmtest)\n\n#&gt; Loading required package: zoo\n\n\n#&gt; \n#&gt; Attaching package: 'zoo'\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n\n\n\n# Load the data\ndata &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/hospdd.rds\")\n\n\n1 Part1\n\n# Define treatment introduction month and hospital threshold\nintroduction_month &lt;- 3.0\ntreatment_threshold_hospital &lt;- 18\n\n# Convert Month and hospital columns into numeric\ndata$month &lt;- as.numeric(data$month)\ndata$hospital &lt;- as.numeric(data$hospital)\n\n# Split the data into treated and controlled groups\ntreated_group &lt;- data %&gt;%\n  filter(hospital &lt;= treatment_threshold_hospital )\n\ncontrol_group &lt;- data %&gt;%\n  filter(hospital &gt; treatment_threshold_hospital )\n\n\nMean Difference between treatment and control group BEFORE treatment \n\nbefore_control_mean &lt;- control_group %&gt;% \n  filter(month &lt;= introduction_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nbefore_treatment_mean  &lt;- treated_group %&gt;% \n  filter(month &lt;= introduction_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nmean_diff_before &lt;- before_treatment_mean - before_control_mean\n\n\n\nMean Difference between treatment and control group AFTER treatment \n\n# Mean Difference between treatment and control group AFTER treatment\nafter_control_mean &lt;- control_group %&gt;% \n  filter(month &gt; introduction_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nafter_treatment_mean &lt;- treated_group %&gt;% \n  filter(month &gt; introduction_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nmean_diff_after &lt;- after_treatment_mean - after_control_mean\n\n# Difference-in-differences\nmean_diff_diff &lt;- mean_diff_after - mean_diff_before\n\n\n\n\n2 Part2\n\nUse linear regression to compute the estimate with group and time fixed effects \n\n# Fit the linear regression model\nmodel &lt;- lm(satis ~ procedure * as.factor(month)+ as.factor(hospital), data)\n\n# Print the results\nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ procedure * as.factor(month) + as.factor(hospital), \n#&gt;     data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1880 -0.4630  0.0024  0.4540  4.3181 \n#&gt; \n#&gt; Coefficients: (3 not defined because of singularities)\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                  3.1716566  0.0562244  56.411  &lt; 2e-16 ***\n#&gt; procedure                    0.8785070  0.0541087  16.236  &lt; 2e-16 ***\n#&gt; as.factor(month)2           -0.0096077  0.0292138  -0.329 0.742261    \n#&gt; as.factor(month)3            0.0219686  0.0292138   0.752 0.452080    \n#&gt; as.factor(month)4            0.0152441  0.0368748   0.413 0.679326    \n#&gt; as.factor(month)5           -0.0246564  0.0368748  -0.669 0.503740    \n#&gt; as.factor(month)6            0.0055796  0.0368748   0.151 0.879734    \n#&gt; as.factor(month)7           -0.0238856  0.0368748  -0.648 0.517169    \n#&gt; as.factor(hospital)2         0.4085664  0.0772468   5.289 1.26e-07 ***\n#&gt; as.factor(hospital)3         0.5336248  0.0793436   6.725 1.88e-11 ***\n#&gt; as.factor(hospital)4         0.2275102  0.0739460   3.077 0.002101 ** \n#&gt; as.factor(hospital)5        -0.1453529  0.0739460  -1.966 0.049375 *  \n#&gt; as.factor(hospital)6         0.4478634  0.0739460   6.057 1.46e-09 ***\n#&gt; as.factor(hospital)7         1.4044164  0.0714606  19.653  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8         0.0718758  0.0763236   0.942 0.346365    \n#&gt; as.factor(hospital)9        -1.5185150  0.0782498 -19.406  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10        1.6828446  0.0772468  21.785  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11        0.2209653  0.0763236   2.895 0.003801 ** \n#&gt; as.factor(hospital)12       -0.0953034  0.0782498  -1.218 0.223287    \n#&gt; as.factor(hospital)13        0.4955931  0.0754708   6.567 5.50e-11 ***\n#&gt; as.factor(hospital)14        0.2330426  0.0793436   2.937 0.003323 ** \n#&gt; as.factor(hospital)15       -0.1444935  0.0793436  -1.821 0.068631 .  \n#&gt; as.factor(hospital)16        1.4142680  0.0772468  18.308  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17        0.4235429  0.0805415   5.259 1.49e-07 ***\n#&gt; as.factor(hospital)18        0.1532761  0.0938225   1.634 0.102369    \n#&gt; as.factor(hospital)19       -0.7453017  0.0811676  -9.182  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20        0.0473874  0.0791192   0.599 0.549234    \n#&gt; as.factor(hospital)21        1.1943370  0.0836287  14.281  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22        0.7993153  0.0823390   9.708  &lt; 2e-16 ***\n#&gt; as.factor(hospital)23        0.7017202  0.0811676   8.645  &lt; 2e-16 ***\n#&gt; as.factor(hospital)24       -0.3081260  0.0866459  -3.556 0.000379 ***\n#&gt; as.factor(hospital)25        0.6464736  0.0927319   6.971 3.41e-12 ***\n#&gt; as.factor(hospital)26        0.2142471  0.0791192   2.708 0.006787 ** \n#&gt; as.factor(hospital)27       -0.3986544  0.0766156  -5.203 2.01e-07 ***\n#&gt; as.factor(hospital)28        0.7119953  0.0836287   8.514  &lt; 2e-16 ***\n#&gt; as.factor(hospital)29        0.2485512  0.0800987   3.103 0.001923 ** \n#&gt; as.factor(hospital)30       -0.1679220  0.0953700  -1.761 0.078324 .  \n#&gt; as.factor(hospital)31        0.5120848  0.0791192   6.472 1.03e-10 ***\n#&gt; as.factor(hospital)32       -0.3233456  0.0800987  -4.037 5.47e-05 ***\n#&gt; as.factor(hospital)33       -0.4539752  0.0791192  -5.738 9.97e-09 ***\n#&gt; as.factor(hospital)34       -0.0004123  0.0746103  -0.006 0.995591    \n#&gt; as.factor(hospital)35        0.3541110  0.0766156   4.622 3.87e-06 ***\n#&gt; as.factor(hospital)36        2.1381425  0.0773862  27.630  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37        0.1404036  0.0927319   1.514 0.130049    \n#&gt; as.factor(hospital)38       -0.0868060  0.0782181  -1.110 0.267124    \n#&gt; as.factor(hospital)39       -0.0234969  0.0823390  -0.285 0.775370    \n#&gt; as.factor(hospital)40        1.1215331  0.0782181  14.339  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41       -0.1497346  0.0766156  -1.954 0.050697 .  \n#&gt; as.factor(hospital)42        0.8811369  0.0850564  10.359  &lt; 2e-16 ***\n#&gt; as.factor(hospital)43       -0.7724325  0.0811676  -9.517  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44        0.0344120  0.0904396   0.380 0.703588    \n#&gt; as.factor(hospital)45       -0.2137495  0.0766156  -2.790 0.005286 ** \n#&gt; as.factor(hospital)46        0.0784915  0.0823390   0.953 0.340484    \n#&gt; procedure:as.factor(month)2         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)3         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)4 -0.0750732  0.0684427  -1.097 0.272731    \n#&gt; procedure:as.factor(month)5  0.0061613  0.0684427   0.090 0.928272    \n#&gt; procedure:as.factor(month)6 -0.0531645  0.0684427  -0.777 0.437317    \n#&gt; procedure:as.factor(month)7         NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7239 on 7312 degrees of freedom\n#&gt; Multiple R-squared:  0.5334, Adjusted R-squared:  0.5299 \n#&gt; F-statistic:   152 on 55 and 7312 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity - Assignment 10",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rddensity)\n\n\n# Read data for the current campaign\ncurrent_campaign_data &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/coupon.rds\")\n# Read data for the different past campaign\npast_campaign_data &lt;- readRDS(\"C:/Users/user/Documents/TUHH/Causal/Causal_Data_Science_Data/shipping.rds\")\n\n\n# [1] Regression Discontinuity Design Sensitivity Analysis ----\n\n# Define cut-off\ncutoff_point &lt;- 60\n\n# Bandwidths\noriginal_bandwidth &lt;- cutoff_point + c(-5, 5)\nhalf_bandwidth &lt;- cutoff_point + c(-5, 5) / 2\ndouble_bandwidth &lt;- cutoff_point + c(-5, 5) * 2\n\n\nFunction to run the regression discontinuity design analysis\n\nrun_rdd_analysis &lt;- function(data, bandwidth) {\n  data_below &lt;- data %&gt;% filter(days_since_last &gt;= bandwidth[1] & days_since_last &lt; cutoff_point)\n  data_above &lt;- data %&gt;% filter(days_since_last &gt;= cutoff_point & days_since_last &lt;= bandwidth[2])\n  data_combined &lt;- bind_rows(data_above, data_below)\n  \n  lm_result &lt;- lm(purchase_after ~ days_since_last_centered + coupon, data_combined)\n  \n  model_below &lt;- lm(purchase_after ~ days_since_last, data_below)\n  model_above &lt;- lm(purchase_after ~ days_since_last, data_above)\n  \n  y0 &lt;- predict(model_below, tibble(days_since_last = cutoff_point))\n  y1 &lt;- predict(model_above, tibble(days_since_last = cutoff_point))\n  \n  late_estimate &lt;- y1 - y0\n  return(list(LATE = late_estimate, Summary = summary(lm_result)))\n}\n\n\n\nRun the analysis with the original bandwidth\n\noriginal_bandwidth_results &lt;- run_rdd_analysis(current_campaign_data, original_bandwidth)\n\n\n\nRun the analysis with half the bandwidth\n\nhalf_bandwidth_results &lt;- run_rdd_analysis(current_campaign_data, half_bandwidth)\n\n\n\nRun the analysis with double the bandwidth\n\ndouble_bandwidth_results &lt;- run_rdd_analysis(current_campaign_data, double_bandwidth)\n\n\n# Print the results for the original bandwidth\ncat(\"Original Bandwidth:\\n\")\n\n#&gt; Original Bandwidth:\n\ncat(\"LATE:\", original_bandwidth_results$LATE, \"\\n\")\n\n#&gt; LATE: 7.989037\n\nprint(original_bandwidth_results$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = data_combined)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.4966  -2.1312  -0.0949   2.0185  10.4159 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.4242     0.3965  28.813  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.3835     0.1259   3.046  0.00251 ** \n#&gt; couponTRUE                 7.9334     0.7087  11.194  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.186 on 320 degrees of freedom\n#&gt; Multiple R-squared:  0.7074, Adjusted R-squared:  0.7055 \n#&gt; F-statistic: 386.8 on 2 and 320 DF,  p-value: &lt; 2.2e-16\n\n# Print the results for half the bandwidth\ncat(\"\\nHalf the Bandwidth:\\n\")\n\n#&gt; \n#&gt; Half the Bandwidth:\n\ncat(\"LATE:\", half_bandwidth_results$LATE, \"\\n\")\n\n#&gt; LATE: 7.362377\n\nprint(half_bandwidth_results$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = data_combined)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9680  -2.2013   0.1676   2.1516   8.2567 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.6612     0.5747  20.292  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.6883     0.3219   2.138   0.0339 *  \n#&gt; couponTRUE                 7.1679     1.0172   7.047 3.87e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.289 on 178 degrees of freedom\n#&gt; Multiple R-squared:  0.6622, Adjusted R-squared:  0.6584 \n#&gt; F-statistic: 174.5 on 2 and 178 DF,  p-value: &lt; 2.2e-16\n\n# Print the results for double the bandwidth\ncat(\"\\nDouble the Bandwidth:\\n\")\n\n#&gt; \n#&gt; Double the Bandwidth:\n\ncat(\"LATE:\", double_bandwidth_results$LATE, \"\\n\")\n\n#&gt; LATE: 9.513531\n\nprint(double_bandwidth_results$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = data_combined)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nThe observed LATE is consistently stable across all three bandwidth choices, implying a consistently positive impact of the coupon variable on purchase_after. While the LATE estimate is slightly reduced for the half bandwidth, indicating a more cautious assessment, it slightly increases for the double bandwidth, suggesting a potential broader impact on individuals farther from the cutoff point. The choice of bandwidth significantly influences the treatment (couponTRUE) effect estimation, with coefficients showing variation in response to changes in bandwidth.\n\n\n\n\n# [2] Different Past Campaign ----\n\nrdd_density_results &lt;- rddensity(past_campaign_data$purchase_amount, c = 30)\nsummary(rdd_density_results)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rdd_density_results): There are repeated\n#&gt; observations. Point estimates and standard errors have been adjusted. Use\n#&gt; option massPoints=FALSE to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\n\n\n\n\n\n\n\nSignificant evidence of manipulation around the cut-off point (c=30) is indicated by the manipulation tests. P-values of the robust T-statistic are nearly zero, implying systematic changes in the observed density of the purchase_amount variable near the cut-off. The order of estimation and bias differs on each side, suggesting a lack of smoothness or continuity in the observed changes. Further evidence of non-random behavior around the cut-off is provided by the p-values of binomial tests. Based on the manipulation testing results, the purchase_amount variable may not be suitable as a running variable for an RDD with a cut-off at 30€.\n\n\n\n\n# Plot to confirm that purchase_amount could not be used as a running variable at 30\nggplot(past_campaign_data, aes(x = purchase_amount)) +\n  geom_histogram(binwidth = 5, fill = \"pink\", color = \"black\") +\n  geom_vline(xintercept = 30, color = \"blue\", linetype = \"dashed\") +\n  xlab(\"Purchase Amount (€)\") +\n  ylab(\"Number of Purchases\") +\n  theme_minimal()"
  }
]